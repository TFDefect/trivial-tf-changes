name: Metrics Collection (S3 Storage)

on:
  push:
    paths:
      - '**.tf'
  pull_request:
    paths:
      - '**.tf'

concurrency:
  group: metrics-collection-${{ github.ref }}
  cancel-in-progress: false

env:
  S3_BUCKET: ${{ secrets.METRICS_S3_BUCKET }}
  S3_PREFIX: metrics

jobs:
  collect-metrics:
    runs-on: ubuntu-latest
    permissions:
      contents: read
      packages: read
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
        with:
          fetch-depth: 0
      
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      
      - name: Install AWS CLI and dependencies
        run: |
          pip install boto3
      
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_REGION || 'us-east-1' }}
      
      - name: Download metrics from S3
        continue-on-error: true
        run: |
          echo "Downloading metrics_history.csv from S3..."
          python scripts/storage_manager.py \
            --file metrics_history.csv \
            --s3-bucket ${{ env.S3_BUCKET }} \
            --s3-prefix ${{ env.S3_PREFIX }} \
            --download
      
      - name: Check if history was downloaded
        run: |
          echo "=== Metrics History Status ==="
          if [ -f "metrics_history.csv" ]; then
            echo "✓ Historical metrics found from S3"
            echo "  Total rows: $(wc -l < metrics_history.csv)"
            echo "  File size: $(du -h metrics_history.csv | cut -f1)"
            echo "  Preview (first 3 rows):"
            head -n 3 metrics_history.csv
          else
            echo "⚠ No historical metrics found in S3"
            echo "  This is the first run or bucket is empty"
          fi
          echo "==========================="
      
      - name: Rotate metrics if needed
        run: |
          if [ -f "metrics_history.csv" ]; then
            python scripts/storage_manager.py \
              --file metrics_history.csv \
              --max-size 10 \
              --keep-rows 500
          fi
      
      - name: Log in to GitHub Container Registry
        uses: docker/login-action@v3
        with:
          registry: ghcr.io
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}
      
      - name: Pull pre-built Docker image
        run: |
          echo "Pulling pre-built image from GitHub Container Registry..."
          if docker pull ghcr.io/${{ github.repository }}:latest 2>/dev/null; then
            docker tag ghcr.io/${{ github.repository }}:latest tf-metrics:latest
            echo "✓ Pre-built image ready"
          else
            echo "⚠ Pre-built image not available (might be building)"
            echo "Building image locally as fallback..."
            docker build -t tf-metrics:latest .
            echo "✓ Local image ready"
          fi
      
      - name: Collect metrics for this commit
        run: |
          set -e
          if [ -f "metrics_history.csv" ]; then
            echo "Using historical context from S3"
            docker run \
              -v ${{ github.workspace }}:/repo \
              -w /repo \
              tf-metrics:latest . \
              --commit ${{ github.sha }} \
              --history metrics_history.csv
          else
            echo "No historical context available"
            docker run \
              -v ${{ github.workspace }}:/repo \
              -w /repo \
              tf-metrics:latest . \
              --commit ${{ github.sha }}
          fi
      
      - name: Upload current metrics to S3 (for prediction)
        if: success() || failure()
        run: |
          if [ -f "metrics.csv" ]; then
            echo "Uploading metrics.csv (current commit only) to S3..."
            aws s3 cp metrics.csv \
              s3://${{ env.S3_BUCKET }}/${{ env.S3_PREFIX }}/current/metrics_${{ github.sha }}.csv
            # Also upload as "latest" for easy access
            aws s3 cp metrics.csv \
              s3://${{ env.S3_BUCKET }}/${{ env.S3_PREFIX }}/current/metrics_latest.csv
            echo "✓ Current metrics uploaded (for prediction)"
          else
            echo "⚠ No metrics.csv found"
          fi
      
      - name: Upload history metrics to S3 (for training)
        if: success() || failure()
        run: |
          if [ -f "metrics_history.csv" ]; then
            echo "Uploading metrics_history.csv (accumulated) to S3..."
            python scripts/storage_manager.py \
              --file metrics_history.csv \
              --s3-bucket ${{ env.S3_BUCKET }} \
              --s3-prefix ${{ env.S3_PREFIX }} \
              --upload
            echo "✓ History upload complete (for training)"
          else
            echo "⚠ No metrics_history.csv to upload"
          fi
      
      - name: Upload archives to S3 (if any)
        if: hashFiles('metrics_archive_*.csv') != ''
        run: |
          for archive in metrics_archive_*.csv; do
            aws s3 cp "$archive" "s3://${{ env.S3_BUCKET }}/${{ env.S3_PREFIX }}/archives/"
          done
      
      - name: Display summary
        run: |
          if [ -f "metrics.csv" ]; then
            echo "✓ Metrics collected and uploaded to S3"
            echo "File size: $(du -h metrics.csv | cut -f1)"
            echo "Row count: $(wc -l < metrics.csv)"
            echo "S3 location: s3://${{ env.S3_BUCKET }}/${{ env.S3_PREFIX }}/metrics.csv"
          fi
